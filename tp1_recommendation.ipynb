{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from string import Template\n",
    "\n",
    "\n",
    "def load_movielens(filename, minidata=False):\n",
    "    \"\"\"\n",
    "    Cette fonction lit le fichier filename de la base de donnees\n",
    "    Movielens, par exemple \n",
    "    filename = '~/datasets/ml-100k/u.data'\n",
    "    Elle retourne \n",
    "    R : une matrice utilisateur-item contenant les scores\n",
    "    mask : une matrice valant 1 si il y a un score et 0 sinon\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.loadtxt(filename, dtype=int)\n",
    "\n",
    "    R = sparse.coo_matrix((data[:, 2], (data[:, 0]-1, data[:, 1]-1)),\n",
    "                          dtype=float)\n",
    "    R = R.toarray()  # not optimized for big data\n",
    "\n",
    "    # code la fonction 1_K\n",
    "    mask = sparse.coo_matrix((np.ones(data[:, 2].shape),\n",
    "                              (data[:, 0]-1, data[:, 1]-1)), dtype=bool )\n",
    "    mask = mask.toarray()  # not optimized for big data\n",
    "\n",
    "    if minidata is True:\n",
    "        R = R[0:100, 0:200].copy()\n",
    "        mask = mask[0:100, 0:200].copy()\n",
    "\n",
    "    return R, mask\n",
    "\n",
    "\n",
    "def objective(P, Q0, R, mask, rho):\n",
    "    \"\"\"\n",
    "    La fonction objectif du probleme simplifie.\n",
    "    Prend en entree \n",
    "    P : la variable matricielle de taille C x I\n",
    "    Q0 : une matrice de taille U x C\n",
    "    R : une matrice de taille U x I\n",
    "    mask : une matrice 0-1 de taille U x I\n",
    "    rho : un reel positif ou nul\n",
    "\n",
    "    Sorties :\n",
    "    val : la valeur de la fonction\n",
    "    grad_P : le gradient par rapport a P\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = (R - Q0.dot(P)) * mask\n",
    "\n",
    "    val = np.sum(tmp ** 2)/2. + rho/2. * (np.sum(Q0 ** 2) + np.sum(P ** 2))\n",
    "\n",
    "    grad_P = rho * P - Q0.T.dot(tmp)\n",
    "\n",
    "    return val, grad_P\n",
    "\n",
    "\n",
    "def total_objective(P, Q, R, mask, rho):\n",
    "    \"\"\"\n",
    "    La fonction objectif du probleme complet.\n",
    "    Prend en entree \n",
    "    P : la variable matricielle de taille C x I\n",
    "    Q : la variable matricielle de taille U x C\n",
    "    R : une matrice de taille U x I\n",
    "    mask : une matrice 0-1 de taille U x I\n",
    "    rho : un reel positif ou nul\n",
    "\n",
    "    Sorties :\n",
    "    val : la valeur de la fonction\n",
    "    grad_P : le gradient par rapport a P\n",
    "    grad_Q : le gradient par rapport a Q\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = (R - Q.dot(P)) * mask\n",
    "\n",
    "    val = np.sum(tmp ** 2)/2. + rho/2. * (np.sum(Q ** 2) + np.sum(P ** 2))\n",
    "\n",
    "    grad_P = rho * P - Q.T.dot(tmp)\n",
    "\n",
    "    grad_Q = rho * Q - tmp.dot(P.T)\n",
    "\n",
    "    return val, grad_P, grad_Q\n",
    "\n",
    "\n",
    "def total_objective_vectorized(PQvec, R, mask, rho):\n",
    "    \"\"\"\n",
    "    Vectorisation de la fonction precedente de maniere a ne pas\n",
    "    recoder la fonction gradient\n",
    "    \"\"\"\n",
    "\n",
    "    # reconstruction de P et Q\n",
    "    n_items = R.shape[1]\n",
    "    n_users = R.shape[0]\n",
    "    F = PQvec.shape[0] / (n_items + n_users)\n",
    "    Pvec = PQvec[0:n_items*F]\n",
    "    Qvec = PQvec[n_items*F:]\n",
    "    P = np.reshape(Pvec, (F, n_items))\n",
    "    Q = np.reshape(Qvec, (n_users, F))\n",
    "\n",
    "    val, grad_P, grad_Q = total_objective(P, Q, R, mask, rho)\n",
    "    return val, np.concatenate([grad_P.ravel(), grad_Q.ravel()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "\n",
    "L'option minidata déduit les données à une dimension 100 x 200 ( contre 943 x 1682 normalement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R, mask = load_movielens(\"u.data\")\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 943 utilisateurs, 1682 films et 100000 notes.\n"
     ]
    }
   ],
   "source": [
    "nbU = R.shape[0]\n",
    "nbFilms = R.shape[1]\n",
    "nbNotes = mask.sum()\n",
    "print(Template(\"Il y a $u utilisateurs, $f films et $n notes.\").substitute(u=nbU, f=nbFilms, n=nbNotes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3\n",
    "On nomme $ f $ la fonction objectif.\n",
    "\n",
    "On calcul le gradient de $ f $ en calculant le gradient par rapport à P puis le gradient par rapport à Q :\n",
    "\n",
    "$ ||P + h||^2 - ||P||^2  = 2tr(^{{\\operatorname t}}\\!P h) + o(h) = <2P, h> + o(h)$\n",
    "\n",
    "$ ||1_K \\circ (R - Q(P + h))||^2 - ||1_K \\circ (R - QP)||^2 = -2tr( ^{{\\operatorname t}}\\!{(1_K \\circ (R - QP))} 1_K \\circ Qh) +o(h) = -2tr( ^{{\\operatorname t}}\\!{(1_K \\circ (R - QP))} Qh) +o(h) = -2tr( ^{{\\operatorname t}}\\!{(^{{\\operatorname t}}\\!Q(1_K \\circ (R - QP)))} h) + o(h) = <-2^{{\\operatorname t}}\\!Q(1_K \\circ (R - Q P)), h> + o(h)$\n",
    "\n",
    "On en déduit que le gradient de $ f $ par rapport à P est (en P, Q): \n",
    "\n",
    "$$ \\rho P - ^{{\\operatorname t}}\\!Q(1_K \\circ (R - Q P)) $$\n",
    "\n",
    "Le calcul du gradient par rapport à Q est similaire.\n",
    "\n",
    "La concaténation des 2 gradients donne : \n",
    "\n",
    "$$ {\\overrightarrow {\\nabla }}f (P, Q)=  \\left(\\rho P - ^{{\\operatorname t}}\\!Q(1_K \\circ (R - Q P)),  \\rho Q - (1_K \\circ (R - Q P)) ^{{\\operatorname t}}\\!P \\right)$$\n",
    "\n",
    "(On ne se prononce pas sur la convexité de $f$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "\n",
    "On déduit facilement de la question précédente que le gradient de g en P est : $ \\rho P - ^{{\\operatorname t}}\\!{Q^0}(1_K \\circ (R - Q^0 P)) $\n",
    "\n",
    "Par ailleurs, $ {\\overrightarrow {\\nabla }^2}g(P) = ^{{\\operatorname t}}\\!{Q^0}Q^0 + \\rho I$ est définie positive donc g est convexe (et même strictement convexe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "\n",
    "Voir au dessus pour l'implémentation en python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "nbC = 4\n",
    "rho = 0.3\n",
    "\n",
    "Q0, _, P0 = svds(R, k=nbC)\n",
    "Q0TQ0 = Q0.T.dot(Q0)\n",
    "gamma = 1. / (rho + math.sqrt(np.sum(Q0TQ0 ** 2)))\n",
    "g = lambda P : objective(P, Q0, R, mask, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réalisé en 26 étapes\n",
      "valeur de la fonction objectif : 369551.54991481936\n"
     ]
    }
   ],
   "source": [
    "def gradient(g, P0, gamma, epsilon, verbose = True, maxIter = 1000):\n",
    "    P = P0.copy()\n",
    "    _, grad = g(P)\n",
    "    nbiter = 0\n",
    "    while(math.sqrt(np.sum(grad ** 2)) > epsilon and nbiter < maxIter):\n",
    "        P_new = P - grad * gamma\n",
    "        P = P_new\n",
    "        _, grad = g(P)\n",
    "        nbiter += 1\n",
    "    if(verbose):\n",
    "        print(\"Réalisé en \"+str(nbiter)+\" étapes\")\n",
    "    return P\n",
    "\n",
    "Pm = gradient(g, P0, gamma, 1)\n",
    "print(\"valeur de la fonction objectif :\", end=\" \")\n",
    "print(g(Pm)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réalisé en 22 étapes\n",
      "valeur de la fonction objectif : 369551.6667174595\n"
     ]
    }
   ],
   "source": [
    "def line_search(g, P, initial_step = 1):\n",
    "    step = initial_step\n",
    "    initial_val, grad = g(P)\n",
    "    val = initial_val + 1\n",
    "    while(val - initial_val > 0):\n",
    "        step /= 2\n",
    "        val, _ = g(P - grad * step)\n",
    "    return step\n",
    "\n",
    "def line_search_gradient(g, P0, epsilon, verbose = True, maxIter = 1000):\n",
    "    P = P0.copy()\n",
    "    _, grad = g(P)\n",
    "    nbiter = 0\n",
    "    while(math.sqrt(np.sum(grad ** 2)) > epsilon and nbiter < maxIter):\n",
    "        gamma = line_search(g, P)\n",
    "        P_new = P - grad * gamma\n",
    "        P = P_new\n",
    "        _, grad = g(P)\n",
    "        nbiter += 1\n",
    "    if(verbose):\n",
    "        print(\"Réalisé en \"+str(nbiter)+\" étapes\")\n",
    "    return P\n",
    "\n",
    "Pm = line_search_gradient(g, P0, 1)\n",
    "print(\"valeur de la fonction objectif :\", end=\" \")\n",
    "print(g(Pm)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2\n",
    "\n",
    "Vérifier que g est quadratique est difficile car son espace de départ est un espace de matrices (et non de vecteurs). Il est donc difficile de mettre g sous la forme : \n",
    "$x \\in \\mathbb {R} ^{|C| \\times |I|}, g(x) = ^{{\\operatorname t}}\\!x.A.x + ^{{\\operatorname t}}\\!b.x + c $ \n",
    "\n",
    "Mais comme $  {\\nabla }^2 g(P) $ ne dépend pas de P et est définie positive, on suppose donc qu'on peut utiliser la méthode des gradients conjugués.\n",
    "\n",
    "De plus $g$ peut se mettre sous la forme : \n",
    "\n",
    "$g(P) = \\frac{1}{2}tr( ^{{\\operatorname t}}\\!P ^{{\\operatorname t}}\\!Q^0 Q^0 P \\circ ^{{\\operatorname t}}\\!1_K 1_K + ^{{\\operatorname t}}\\!P \\rho I P) + tr(^{{\\operatorname t}}\\!B P) + c $ \n",
    "\n",
    "Cette forme laisse penser qu'une fois vectoriser, on pourra mettre g sous forme quadratique\n",
    "\n",
    "Comme on a pas mis en évidence la matrice $A$, on utilisera $tr( ^{{\\operatorname t}}\\!X ^{{\\operatorname t}}\\!Q^0 Q^0 Y \\circ ^{{\\operatorname t}}\\!1_K 1_K + ^{{\\operatorname t}}\\!X \\rho I Y)$ à la place de $^{{\\operatorname t}}\\!x.A.y$ dans l'implémentation de l'algorithme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conjugate_gradient(q, x0, Q0, rho, partial):\n",
    "    global mask\n",
    "    tmp = mask.T.dot(mask) \n",
    "    Q0TQ0 = Q0.T.dot(Q0)\n",
    "    scal = lambda v, w : v.T.dot(w).trace()\n",
    "    scalA = lambda v, w : (v.T.dot(Q0TQ0).dot(w) * tmp).trace() + rho * scal(v, w)\n",
    "    \n",
    "    x = x0.copy()\n",
    "    _, g = q(x)\n",
    "    d = - g\n",
    "    s = - scal(d, g) / scalA(d, d)\n",
    "    x += s * d\n",
    "    n = Q0.shape[1]\n",
    "    \n",
    "    B = x0.shape[0] * x0.shape[1]\n",
    "    if(partial):\n",
    "        B = 20\n",
    "    \n",
    "    for k in range(0, B):\n",
    "        _, g = q(x)\n",
    "        b = scalA(d, g) / scalA(d, d)\n",
    "        d = - g + b * d\n",
    "        s = - scal(d, g) / scalA(d, d)\n",
    "        x += s * d\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- ATTENTION : long ( > 10 minutes) -----\n",
    "Pmconj = conjugate_gradient(g, P0, Q0, rho, False)\n",
    "g(Pmconj)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'exécution complète de l'algorithme des gradients conjugués est très longue (de l'ordre de 10 minutes) car la dimension de l'espace de départ de g est très grande : 6728 ( hauteur de P $ \\times $ largeur de P)\n",
    "\n",
    "Le paramètre partial = True permet de s'arréter à la 20 itérations. \n",
    "\n",
    "L'éxécution complète donne un valeur à 369550.59571634443.\n",
    "Cette valeur (à erreur d'arrondi numérique prêt) est supposée être le minimum de g. \n",
    "Ce que l'on vérifie en effectuant des itérations supplémentaires à partir du P trouvé : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pm = conjugate_gradient(g, Pmconj, Q0, rho, True)\n",
    "g(Pm)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La valeur ne change pas du tout. On peut donc penser que l'on a bien trouvé le minimum de P (à erreur d'arrondi numérique prêt) et que g est effectivement quadratique comme on l'a supposé.\n",
    "\n",
    "Comparée aux deux autres méthodes, cette méthode est bien plus longue. Comme l'écart relatif entre les valeurs de la fonction objectif sont faibles, on pourra se contenter d'utiliser une méthode du gradient (avec recherche linéaire ou non) ou utilisé la méthode des gradients conjugués avec un nombre restreint d'itération.\n",
    "Comme ci-dessous : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pm = conjugate_gradient(g, P0, Q0, rho, True)\n",
    "g(Pm)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2\n",
    "\n",
    "La première ligne dans la boucle for se traduit par :\n",
    "$$ \\forall P, f(P_k, Q_{k-1}) \\leq f(P, Q_{k-1}) $$\n",
    "La deuxième ligne se traduit par :\n",
    "$$ \\forall Q, f(P_k, Q_k) \\leq f(P_k, Q) $$\n",
    "\n",
    "En particulier :\n",
    "$$\n",
    "{\\begin{cases}\n",
    "f(P_k, Q_{k-1}) \\leq f(P_{k-1}, Q_{k-1})\n",
    "\\\\\n",
    "f(P_k, Q_k) \\leq f(P_k, Q_{k-1})\n",
    "\\end{cases}}\n",
    "$$\n",
    "\n",
    "En combiant ces deux inégalités, on obtient que $f(P_k, Q_k) \\leq f(P_{k-1}, Q_{k-1})$ et donc la suite formée par les $f(P_k, Q_k) $ est décroissante. Comme elle est aussi minorée ($f$ admet un minimum), on en conclut qu'elle converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3\n",
    "\n",
    "Pour des raisons pratiques, on définit une fonction python objective_bis qui correspond à :\n",
    "$$ h(Q) = \\frac{1}{2}||1_K \\circ (R - QP_0)||^2 + \\frac{\\rho}{2}||P_0||^2 + \\frac{\\rho}{2}||Q||^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_bis(P0, Q, R, mask, rho):\n",
    "    tmp = (R - Q.dot(P0)) * mask\n",
    "\n",
    "    val = np.sum(tmp ** 2)/2. + rho/2. * (np.sum(Q ** 2) + np.sum(P0 ** 2))\n",
    "\n",
    "    grad_Q = rho * Q - tmp.dot(P0.T)\n",
    "\n",
    "    return val, grad_Q\n",
    "\n",
    "def alternate_least_square(P0, Q0, epsilon, nbIter):\n",
    "    global R, mask, rho\n",
    "    P, Q = P0.copy(), Q0.copy()\n",
    "    for k in range(nbIter):\n",
    "        gP = lambda P : objective(P, Q, R, mask, rho)\n",
    "        P = line_search_gradient(gP, P, epsilon, False, 30)\n",
    "        gQ = lambda Q : objective_bis(P, Q, R, mask, rho)\n",
    "        Q = line_search_gradient(gQ, Q, epsilon, False, 30)\n",
    "    return P, Q\n",
    "\n",
    "#prend + de 2 minutes\n",
    "Pm, Qm = alternate_least_square(P0, Q0, 1, 10)\n",
    "print(total_objective(Pm, Qm, R, mask, rho)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
